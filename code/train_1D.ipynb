{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# Load the model\n",
    "model_name = \"roberta-large\"\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/wiki-labeled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = df[df['label'] == 0]\n",
    "human = df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt.sample(n=150)\n",
    "human = human.sample(n=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        #embeddings = torch.mean(outputs.last_hidden_state, dim=1)  # Mean pooling of token embeddings\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        embeddings = last_hidden_states[:,0,:]\n",
    "    return embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def remove_outliers(embeddings):\n",
    "        # Calculate the mode of the embeddings\n",
    "        mode_val = np.mean(embeddings[0], axis=None)\n",
    "        \n",
    "        # Replace outliers with the mode value\n",
    "        embeddings[np.abs(embeddings - mode_val) > 2 * np.std(embeddings)] = mode_val\n",
    "        \n",
    "        return embeddings\n",
    "def min_max_normalize(embeddings):\n",
    "    # Find the minimum and maximum values in the embeddings\n",
    "    min_val = np.min(embeddings)\n",
    "    max_val = np.max(embeddings)\n",
    "    \n",
    "    # Normalize the embeddings to range [0, 255]\n",
    "    normalized_embeddings = 255 * (embeddings - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return normalized_embeddings.astype(np.uint8)  # Convert to uint8 for integer values between 0 and 255\n",
    "\n",
    "def reshape_embeddings(embeddings):\n",
    "    # Reshape the embeddings to 3D array\n",
    "    return embeddings.reshape(32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt['embeddings'] = gpt['text'].apply(generate_bert_embeddings)\n",
    "gpt['normalized_embeddings'] = gpt['embeddings'].apply(remove_outliers)\n",
    "gpt['normalized_embeddings1'] = gpt['normalized_embeddings'].apply(min_max_normalize)\n",
    "gpt['reshaped_embeddings'] = gpt['normalized_embeddings1'].apply(reshape_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "human['embeddings'] = human['text'].apply(generate_bert_embeddings)\n",
    "human['normalized_embeddings'] = human['embeddings'].apply(remove_outliers)\n",
    "human['normalized_embeddings1'] = human['normalized_embeddings'].apply(min_max_normalize)\n",
    "human['reshaped_embeddings'] = human['normalized_embeddings1'].apply(reshape_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_embed = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/embeddings/gpt_embeddings_wiki.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_embed = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/embeddings/human_embeddings_wiki.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_embed1 = list(gpt_embed)\n",
    "human_embed1 = list(human_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_596170/3765168792.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  gpt_embed = torch.tensor(gpt['normalized_embeddings'].tolist())\n"
     ]
    }
   ],
   "source": [
    "gpt_embed = torch.tensor(gpt['normalized_embeddings'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_embed = torch.tensor(human['normalized_embeddings'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_605992/1006583565.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  gpt_embed_tensor = torch.tensor(gpt_embed1)\n"
     ]
    }
   ],
   "source": [
    "gpt_embed_tensor = torch.tensor(gpt_embed1)\n",
    "human_embed_tensor = torch.tensor(human_embed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print((gpt_embed.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(gpt_embed[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors with zeros and ones\n",
    "zeros_tensor = torch.zeros(15000)\n",
    "ones_tensor = torch.ones(15000)\n",
    "\n",
    "# Concatenate the tensors along the first dimension\n",
    "labels = torch.cat((zeros_tensor, ones_tensor), dim=0)\n",
    "\n",
    "# Display the result tensor\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = torch.cat((gpt_embed_tensor, human_embed_tensor), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "test_size = 0.2\n",
    "dataset = TensorDataset(embeds, labels)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=test_size, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 8, kernel_size=16, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=8, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(8, 16, kernel_size=8, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, stride=2)\n",
    "\n",
    "        # self.conv3 = nn.Conv2d(4, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # self.relu3 = nn.ReLU()\n",
    "        # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(1920, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        # x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.relu5(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN(in_channels=1, num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1498\n",
      "Validation Accuracy: 0.9838\n",
      "Epoch [2/100], Loss: 0.0630\n",
      "Validation Accuracy: 0.9883\n",
      "Epoch [3/100], Loss: 0.0442\n",
      "Validation Accuracy: 0.9863\n",
      "Epoch [4/100], Loss: 0.0395\n",
      "Validation Accuracy: 0.9792\n",
      "Epoch [5/100], Loss: 0.0396\n",
      "Validation Accuracy: 0.9907\n",
      "Epoch [6/100], Loss: 0.0350\n",
      "Validation Accuracy: 0.9898\n",
      "Epoch [7/100], Loss: 0.0319\n",
      "Validation Accuracy: 0.9827\n",
      "Epoch [8/100], Loss: 0.0294\n",
      "Validation Accuracy: 0.9882\n",
      "Epoch [9/100], Loss: 0.0333\n",
      "Validation Accuracy: 0.9903\n",
      "Epoch [10/100], Loss: 0.0287\n",
      "Validation Accuracy: 0.9935\n",
      "Epoch [11/100], Loss: 0.0253\n",
      "Validation Accuracy: 0.9922\n",
      "Epoch [12/100], Loss: 0.0277\n",
      "Validation Accuracy: 0.9923\n",
      "Epoch [13/100], Loss: 0.0243\n",
      "Validation Accuracy: 0.9923\n",
      "Epoch [14/100], Loss: 0.0233\n",
      "Validation Accuracy: 0.9820\n",
      "Epoch [15/100], Loss: 0.0263\n",
      "Validation Accuracy: 0.9908\n",
      "Epoch [16/100], Loss: 0.0236\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [17/100], Loss: 0.0199\n",
      "Validation Accuracy: 0.9900\n",
      "Epoch [18/100], Loss: 0.0203\n",
      "Validation Accuracy: 0.9927\n",
      "Epoch [19/100], Loss: 0.0201\n",
      "Validation Accuracy: 0.9907\n",
      "Epoch [20/100], Loss: 0.0204\n",
      "Validation Accuracy: 0.9920\n",
      "Epoch [21/100], Loss: 0.0205\n",
      "Validation Accuracy: 0.9923\n",
      "Epoch [22/100], Loss: 0.0193\n",
      "Validation Accuracy: 0.9913\n",
      "Epoch [23/100], Loss: 0.0174\n",
      "Validation Accuracy: 0.9940\n",
      "Epoch [24/100], Loss: 0.0171\n",
      "Validation Accuracy: 0.9942\n",
      "Epoch [25/100], Loss: 0.0157\n",
      "Validation Accuracy: 0.9925\n",
      "Epoch [26/100], Loss: 0.0142\n",
      "Validation Accuracy: 0.9915\n",
      "Epoch [27/100], Loss: 0.0143\n",
      "Validation Accuracy: 0.9923\n",
      "Epoch [28/100], Loss: 0.0154\n",
      "Validation Accuracy: 0.9930\n",
      "Epoch [29/100], Loss: 0.0141\n",
      "Validation Accuracy: 0.9928\n",
      "Epoch [30/100], Loss: 0.0141\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [31/100], Loss: 0.0131\n",
      "Validation Accuracy: 0.9923\n",
      "Epoch [32/100], Loss: 0.0126\n",
      "Validation Accuracy: 0.9927\n",
      "Epoch [33/100], Loss: 0.0111\n",
      "Validation Accuracy: 0.9930\n",
      "Epoch [34/100], Loss: 0.0127\n",
      "Validation Accuracy: 0.9927\n",
      "Epoch [35/100], Loss: 0.0122\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [36/100], Loss: 0.0131\n",
      "Validation Accuracy: 0.9938\n",
      "Epoch [37/100], Loss: 0.0097\n",
      "Validation Accuracy: 0.9873\n",
      "Epoch [38/100], Loss: 0.0107\n",
      "Validation Accuracy: 0.9928\n",
      "Epoch [39/100], Loss: 0.0087\n",
      "Validation Accuracy: 0.9943\n",
      "Epoch [40/100], Loss: 0.0101\n",
      "Validation Accuracy: 0.9758\n",
      "Epoch [41/100], Loss: 0.0098\n",
      "Validation Accuracy: 0.9897\n",
      "Epoch [42/100], Loss: 0.0100\n",
      "Validation Accuracy: 0.9747\n",
      "Epoch [43/100], Loss: 0.0115\n",
      "Validation Accuracy: 0.9942\n",
      "Epoch [44/100], Loss: 0.0073\n",
      "Validation Accuracy: 0.9933\n",
      "Epoch [45/100], Loss: 0.0079\n",
      "Validation Accuracy: 0.9893\n",
      "Epoch [46/100], Loss: 0.0077\n",
      "Validation Accuracy: 0.9893\n",
      "Epoch [47/100], Loss: 0.0097\n",
      "Validation Accuracy: 0.9898\n",
      "Epoch [48/100], Loss: 0.0079\n",
      "Validation Accuracy: 0.9925\n",
      "Epoch [49/100], Loss: 0.0092\n",
      "Validation Accuracy: 0.9947\n",
      "Epoch [50/100], Loss: 0.0061\n",
      "Validation Accuracy: 0.9908\n",
      "Epoch [51/100], Loss: 0.0086\n",
      "Validation Accuracy: 0.9922\n",
      "Epoch [52/100], Loss: 0.0064\n",
      "Validation Accuracy: 0.9878\n",
      "Epoch [53/100], Loss: 0.0083\n",
      "Validation Accuracy: 0.9943\n",
      "Epoch [54/100], Loss: 0.0065\n",
      "Validation Accuracy: 0.9952\n",
      "Epoch [55/100], Loss: 0.0069\n",
      "Validation Accuracy: 0.9907\n",
      "Epoch [56/100], Loss: 0.0052\n",
      "Validation Accuracy: 0.9938\n",
      "Epoch [57/100], Loss: 0.0073\n",
      "Validation Accuracy: 0.9925\n",
      "Epoch [58/100], Loss: 0.0063\n",
      "Validation Accuracy: 0.9938\n",
      "Epoch [59/100], Loss: 0.0072\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [60/100], Loss: 0.0066\n",
      "Validation Accuracy: 0.9930\n",
      "Epoch [61/100], Loss: 0.0057\n",
      "Validation Accuracy: 0.9920\n",
      "Epoch [62/100], Loss: 0.0070\n",
      "Validation Accuracy: 0.9943\n",
      "Epoch [63/100], Loss: 0.0064\n",
      "Validation Accuracy: 0.9948\n",
      "Epoch [64/100], Loss: 0.0054\n",
      "Validation Accuracy: 0.9943\n",
      "Epoch [65/100], Loss: 0.0048\n",
      "Validation Accuracy: 0.9918\n",
      "Epoch [66/100], Loss: 0.0046\n",
      "Validation Accuracy: 0.9955\n",
      "Epoch [67/100], Loss: 0.0066\n",
      "Validation Accuracy: 0.9947\n",
      "Epoch [68/100], Loss: 0.0041\n",
      "Validation Accuracy: 0.9932\n",
      "Epoch [69/100], Loss: 0.0048\n",
      "Validation Accuracy: 0.9947\n",
      "Epoch [70/100], Loss: 0.0060\n",
      "Validation Accuracy: 0.9917\n",
      "Epoch [71/100], Loss: 0.0071\n",
      "Validation Accuracy: 0.9832\n",
      "Epoch [72/100], Loss: 0.0034\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [73/100], Loss: 0.0071\n",
      "Validation Accuracy: 0.9937\n",
      "Epoch [74/100], Loss: 0.0033\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [75/100], Loss: 0.0091\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [76/100], Loss: 0.0036\n",
      "Validation Accuracy: 0.9938\n",
      "Epoch [77/100], Loss: 0.0033\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [78/100], Loss: 0.0038\n",
      "Validation Accuracy: 0.9937\n",
      "Epoch [79/100], Loss: 0.0061\n",
      "Validation Accuracy: 0.9955\n",
      "Epoch [80/100], Loss: 0.0041\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [81/100], Loss: 0.0040\n",
      "Validation Accuracy: 0.9940\n",
      "Epoch [82/100], Loss: 0.0043\n",
      "Validation Accuracy: 0.9937\n",
      "Epoch [83/100], Loss: 0.0045\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [84/100], Loss: 0.0044\n",
      "Validation Accuracy: 0.9932\n",
      "Epoch [85/100], Loss: 0.0032\n",
      "Validation Accuracy: 0.9948\n",
      "Epoch [86/100], Loss: 0.0063\n",
      "Validation Accuracy: 0.9942\n",
      "Epoch [87/100], Loss: 0.0020\n",
      "Validation Accuracy: 0.9912\n",
      "Epoch [88/100], Loss: 0.0042\n",
      "Validation Accuracy: 0.9918\n",
      "Epoch [89/100], Loss: 0.0070\n",
      "Validation Accuracy: 0.9927\n",
      "Epoch [90/100], Loss: 0.0028\n",
      "Validation Accuracy: 0.9938\n",
      "Epoch [91/100], Loss: 0.0036\n",
      "Validation Accuracy: 0.9950\n",
      "Epoch [92/100], Loss: 0.0043\n",
      "Validation Accuracy: 0.9892\n",
      "Epoch [93/100], Loss: 0.0039\n",
      "Validation Accuracy: 0.9952\n",
      "Epoch [94/100], Loss: 0.0030\n",
      "Validation Accuracy: 0.9945\n",
      "Epoch [95/100], Loss: 0.0031\n",
      "Validation Accuracy: 0.9933\n",
      "Epoch [96/100], Loss: 0.0050\n",
      "Validation Accuracy: 0.9938\n",
      "Epoch [97/100], Loss: 0.0029\n",
      "Validation Accuracy: 0.9907\n",
      "Epoch [98/100], Loss: 0.0051\n",
      "Validation Accuracy: 0.9900\n",
      "Epoch [99/100], Loss: 0.0048\n",
      "Validation Accuracy: 0.9910\n",
      "Epoch [100/100], Loss: 0.0062\n",
      "Validation Accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-2)\n",
    "\n",
    "# Step 5: Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            #print(labels, predicted)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model_path = \"/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/model/wiki_model_1D.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
