{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model, pipeline, AutoModelForCausalLM, \\\n",
    "AutoTokenizer, BitsAndBytesConfig, LlamaTokenizer, LlamaModel, AutoModelForTextEncoding\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import random\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GPT_CNN2D(nn.Module):\n",
    "    def __init__(self, embedding_model):\n",
    "        super(GPT_CNN2D, self).__init__()\n",
    "        self.em_model = embedding_model\n",
    "        \n",
    "            \n",
    "        if embedding_model == \"custom\":\n",
    "            self.conv1 = nn.Conv2d(1, 128, kernel_size=8, padding=1, stride = 2)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "            self.conv2 = nn.Conv2d(128, 64, kernel_size=5, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "            self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "            self.flatten = nn.Flatten()            \n",
    "            # self.fc1 = nn.Linear(96, 2)\n",
    "            self.fc1 = nn.Linear(384, 2)\n",
    "            # self.fc2 = nn.Linear(128, 2)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.em_model == \"custom\":\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.flatten(x)\n",
    "            # x = self.relu(self.fc1(x))\n",
    "            x = self.fc1(x)\n",
    "            # x = self.sigmoid(self.fc2(x))\n",
    "            return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT_CNN2D(embedding_model = \"custom\").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "# Create an instance of the model\n",
    "# Map the location to the first CUDA device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_weights = torch.load('/home/csgrad/kaushik3/LLM/Code/Embedding_Fusion/Code/gpt_BERT_FLAN.pth', map_location=device)\n",
    "model.load_state_dict(pretrained_weights)\n",
    "\n",
    "\n",
    "# Now the model is loaded with the pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yaful/DeepfakeTextDetect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15820 1830\n",
      "1830 1830\n",
      "228 228\n",
      "220 220\n"
     ]
    }
   ],
   "source": [
    "human_train = [data for data in dataset['train'] if data['src'] == 'squad_human']\n",
    "human_test = [data for data in dataset['test'] if data['src'] == 'squad_human']\n",
    "human_val = [data for data in dataset['validation'] if data['src'] == 'squad_human']\n",
    "\n",
    "llm_train = [data for data in dataset['train'] if 'gpt' in data['src'] and 'squad' in data['src']]\n",
    "llm_test = [data for data in dataset['test'] if 'gpt' in data['src'] and 'squad' in data['src']]\n",
    "llm_val = [data for data in dataset['validation'] if 'gpt' in data['src'] and 'squad' in data['src']]\n",
    "\n",
    "import random\n",
    "\n",
    "min_train_length = min(len(human_train), len(llm_train))\n",
    "min_test_length = min(len(human_test), len(llm_test))\n",
    "min_val_length = min(len(human_val), len(llm_val))\n",
    "\n",
    "squad_human_train = random.sample(human_train, min_train_length)\n",
    "squad_human_test = random.sample(human_test, min_test_length)\n",
    "squad_human_val = random.sample(human_val, min_val_length)\n",
    "\n",
    "squad_gpt_train = random.sample(llm_train, min_train_length)\n",
    "squad_gpt_test = random.sample(llm_train, min_test_length)\n",
    "squad_gpt_val = random.sample(llm_val, min_val_length)\n",
    "\n",
    "print(len(human_train), len(llm_train))\n",
    "print(len(squad_human_train), len(squad_gpt_train))\n",
    "print(len(squad_human_test), len(squad_gpt_test))\n",
    "print(len(squad_human_val), len(squad_gpt_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Human = pd.DataFrame()\n",
    "squad_human = squad_human_train + squad_human_val + squad_human_test\n",
    "Human['text'] = [data['text'] for data in squad_human]\n",
    "\n",
    "AI = pd.DataFrame()\n",
    "squad_gpt = squad_gpt_train + squad_gpt_val + squad_gpt_test\n",
    "AI['text'] = [data['text'] for data in squad_gpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Human = Human['text'].tolist()\n",
    "AI = AI['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gratitude for all these resources and the determination to develop oneself would be more productive than criticism and blame because the resources are readily available and because, if you blame others, there is no need for you to do something different tomorrow or for you to change and improve. Where there is a will, there is a way. People in developed countries have the will and the way to do many things that they want to do. They sometimes need more determination and will to improve and to educate themselves with the resources that are abundantly available. They occasionally need more gratitude for the resources they have, including their teachers and their textbooks. The entire internet is also available to supplement these teachers and textbooks.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Human[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to('cuda')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "input_ids = tokenizer.encode(Human[0], return_tensors='pt',padding=True, truncation=True, max_length=512).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 137])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492828c680c849e998d9485304ffdf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model_gpt = GPT2Model.from_pretrained('gpt2') \n",
    "model_gpt = model_gpt.to('cuda')\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_name = \"roberta-large\"\n",
    "model_bert = RobertaModel.from_pretrained(model_name)\n",
    "model_bert = model_bert.to('cuda')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_bert = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_flan = AutoTokenizer.from_pretrained(\"/data/kaushik3/models--pszemraj--flan-ul2-text-encoder/snapshots/1ffcec2cc23aa34ea6edb96f86c12f179ee9e87f\")\n",
    "model_flan = AutoModelForTextEncoding.from_pretrained(\"/data/kaushik3/models--pszemraj--flan-ul2-text-encoder/snapshots/1ffcec2cc23aa34ea6edb96f86c12f179ee9e87f\")\n",
    "model_flan = model_flan.to('cuda')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to('cuda')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}) \n",
    "\n",
    "for val in range(len(Human)):\n",
    "    \n",
    "    if val%100==0:\n",
    "        print(val)\n",
    "    \n",
    "    input_ids = tokenizer.encode(Human[val], return_tensors='pt',padding=True, truncation=True, max_length=512).to(device) \n",
    "    #inputs = {name: tensor.to('cuda') for name, tensor in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        #embeddings = torch.mean(outputs.last_hidden_state, dim=1)  # Mean pooling of token embeddings\n",
    "        #last_hidden_states = outputs.last_hidden_state\n",
    "        embeddings = outputs.last_hidden_state.mean(dim = 1).squeeze().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278 2278 2278 2278 2278 2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1033269/3304321486.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  human_bert = torch.tensor(human_bert)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "human_bert = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/code.old/Deepfake_dataset/Squad/BERT/human_embeddings.npy', allow_pickle=True)\n",
    "human_gpt = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/code.old/Deepfake_dataset/Squad/GPT/human_embeddings.npy', allow_pickle=True)\n",
    "human_flan = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/code.old/Deepfake_dataset/Squad/FLAN/human_embeddings.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "ai_bert = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/code.old/Deepfake_dataset/Squad/BERT/gpt_embeddings.npy', allow_pickle=True)\n",
    "ai_gpt = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/code.old/Deepfake_dataset/Squad/GPT/gpt_embeddings.npy', allow_pickle=True)\n",
    "ai_flan = np.load('/home/csgrad/sunilruf/detect_llm/sunil_code/LLM/code.old/Deepfake_dataset/Squad/FLAN/gpt_embeddings.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "bert_min_length = min(len(human_bert), len(ai_bert))\n",
    "human_bert = human_bert[:bert_min_length]\n",
    "ai_bert = ai_bert[:bert_min_length]\n",
    "\n",
    "gpt_min_length = min(len(human_gpt), len(ai_gpt))\n",
    "human_gpt = human_gpt[:bert_min_length]\n",
    "ai_gpt = ai_gpt[:bert_min_length]\n",
    "\n",
    "flan_min_length = min(len(human_flan), len(ai_flan))\n",
    "human_flan = human_flan[:bert_min_length]\n",
    "ai_flan = ai_flan[:bert_min_length]\n",
    "\n",
    "\n",
    "print(len(human_bert), len(human_gpt), len(human_flan), len(ai_bert), len(ai_gpt), len(ai_flan))\n",
    "\n",
    "human_bert = list(human_bert)\n",
    "ai_bert = list(ai_bert)\n",
    "human_gpt = list(human_gpt)\n",
    "ai_gpt = list(ai_gpt)\n",
    "human_flan = list(human_flan)\n",
    "ai_flan = list(ai_flan)\n",
    "\n",
    "human_bert = torch.tensor(human_bert)\n",
    "ai_bert = torch.tensor(ai_bert)\n",
    "human_gpt = torch.tensor(human_gpt).reshape([bert_min_length, 1, 768])\n",
    "ai_gpt = torch.tensor(ai_gpt).reshape([bert_min_length, 1, 768])\n",
    "human_flan = torch.tensor(human_flan).reshape([bert_min_length, 1, 4096])\n",
    "ai_flan = torch.tensor(ai_flan).reshape([bert_min_length, 1, 4096])\n",
    "\n",
    "\n",
    "human_bert = human_bert.view(bert_min_length, 16,64)\n",
    "human_gpt = human_gpt.view(bert_min_length, 12,64)\n",
    "human_flan = human_flan.view(bert_min_length, 64,64)\n",
    "\n",
    "ai_bert = ai_bert.view(bert_min_length, 16,64)\n",
    "ai_gpt = ai_gpt.view(bert_min_length, 12,64)\n",
    "ai_flan = ai_flan.view(bert_min_length, 64,64)\n",
    "\n",
    "human = torch.cat([human_bert, human_gpt, human_flan], dim=1)\n",
    "ai = torch.cat([ai_bert, ai_gpt, ai_flan], dim=1)\n",
    "embeds = torch.cat((human, ai), dim=0)\n",
    "zeros_tensor = torch.zeros((bert_min_length))\n",
    "ones_tensor = torch.ones((bert_min_length))\n",
    "\n",
    "# Concatenate the tensors along the first dimension\n",
    "labels = torch.cat((zeros_tensor, ones_tensor), dim=0)\n",
    "\n",
    "# Display the result tensor\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "test_size = 0.2\n",
    "dataset = TensorDataset(embeds, labels)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=test_size, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x480 and 384x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m TP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/bio3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m, in \u001b[0;36mGPT_CNN2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# x = self.relu(self.fc1(x))\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# x = self.sigmoid(self.fc2(x))\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/bio3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bio3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x480 and 384x2)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = GPT_CNN2D(embedding_model = \"fusion3\").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "max_val_accuracy = 0\n",
    "# Step 5: Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            TP += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            FP += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            TN += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "            FN += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "            #print(labels, predicted)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    if max_val_accuracy<val_accuracy:\n",
    "        max_TP = TP\n",
    "        max_FP = FP\n",
    "        max_TN = TN\n",
    "        max_FN = FN\n",
    "    max_val_accuracy = max(max_val_accuracy, val_accuracy)\n",
    "    \n",
    "    \n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
